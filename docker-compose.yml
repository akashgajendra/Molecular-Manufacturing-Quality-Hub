services:
  postgres:
    image: postgres:16-alpine
    container_name: postgres_db
    restart: always
    env_file:
      - .env
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"] # Checks if the DB is ready
      interval: 5s
      timeout: 5s
      retries: 5
  
  db_init:
    build:
      context: ./backend/database 
    container_name: db_init
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env

  minio:
      image: minio/minio
      container_name: minio_storage
      restart: always
      ports:
        - "9000:9000"
        - "9001:9001" # MinIO Console Port
      env_file:
        - .env
      volumes:
        - minio_data:/data
      
      entrypoint: >
        /bin/sh -c "
        /usr/bin/minio server /data --console-address ':9001' &
        sleep 5;
        /usr/bin/mc alias set local http://localhost:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD;
        /usr/bin/mc mb local/quality-hub-dev;
        wait
        "

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # IMPORTANT: Ensure Kafka listeners use the internal network name 'kafka' for brokers
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"      
      # ---------------------
    healthcheck:
      test: ["CMD", "bash", "-c", "kafka-topics --bootstrap-server kafka:29092 --list"]
      interval: 10s
      timeout: 5s
      retries: 10

  kafka_init:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka_init
    depends_on:
      - kafka
    volumes:
      # Mount the script into the container
      - ./create_topics.sh:/tmp/create_topics.sh
    command: ["/bin/bash", "/tmp/create_topics.sh"]
    environment:
      # Use the internal listener for the script connection
      KAFKA_BROKER_LISTENER: kafka:29092

  kafka_ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka_ui
    ports:
      - "8080:8080"
    depends_on:
      kafka:
        condition: service_healthy # Wait for Kafka to be healthy (and topics created)
    environment:
      # KAFKA_CLUSTERS_0_NAME: The name displayed in the UI
      KAFKA_CLUSTERS_0_NAME: 'Molecular-Manufacturing-Hub'
      # KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: Must use the internal service name and the internal listener port (29092)
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: 'kafka:29092' 
      # KAFKA_CLUSTERS_0_ZOOKEEPER: The internal service name and port of your Zookeeper
      KAFKA_CLUSTERS_0_ZOOKEEPER: 'zookeeper:2181'
      # Set to 'true' to allow creating/deleting topics and viewing messages
      KAFKA_CLUSTERS_0_AUDIT_TOPIC_ENABLED: true
  # -------------------------------------
  # 2. APPLICATION SERVICES
  # -------------------------------------

  api:
    build:
      context: ./backend/api
    container_name: api_gateway
    command: uvicorn main:app --host 0.0.0.0 --port 8000
    volumes:
      - ./backend/api:/app
    ports:
      - "8000:8000"
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
      postgres:
        condition: service_started
      db_init:
        condition: service_completed_successfully
    env_file:
      - .env

  peptide_qc_worker:
    build:
      context: ./backend/workers/peptide_qc
    container_name: peptide_qc_worker
    command: python consumer.py 
    volumes:
      - ./backend/workers/peptide_qc:/app
      - ./data/peptide_qc_data:/data/raw
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
      postgres:
        condition: service_started
      db_init:
        condition: service_completed_successfully
    env_file:
      - .env
    restart: on-failure
    
  colony_counter_worker:
    build:
      context: ./backend/workers/colony_counter
    container_name: colony_counter_worker
    command: python consumer.py
    volumes:
      - ./backend/workers/colony_counter:/app
      - ./data/colony_counter_data:/data/raw
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
      postgres:
        condition: service_started
      db_init:
        condition: service_completed_successfully
    env_file:
      - .env
    restart: on-failure

  # --- FINALIZED CRISPR GENOMICS WORKER ---
  crispr_genomics_worker:
    # NOTE: The Bowtie2 installation and genome download is handled by the Dockerfile 
    #       inside this build context.
      build:
        # Context must be the worker's folder to find its specific Dockerfile and requirements.txt
        context: ./backend/workers/crispr_genomics 
      container_name: crispr_genomics_worker
      # command: python consumer.py
      volumes:
        # Mount the worker code for live development
        - ./backend/workers/crispr_genomics:/app 
        # Mount the entire project root volume as /data if Bowtie2 needs access to a large volume later.
        # However, for the current setup, the genome index is downloaded inside the image.
      depends_on:
        kafka:
          condition: service_healthy
        postgres:
          condition: service_started
        db_init:
          condition: service_completed_successfully
      env_file:
        - .env
      # We remove 'minio' dependency as the core Bowtie2 analysis doesn't need file storage
      # If the job input (gRNA) were a huge file, it would be needed.
      restart: on-failure

volumes:
  minio_data: